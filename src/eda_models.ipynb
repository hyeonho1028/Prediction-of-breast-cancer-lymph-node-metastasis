{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from utils import obj\n",
    "from sdv.tabular.ctgan import CTGAN\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "pd.options.display.max_columns=100\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from utils import seed_everything\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../open/'\n",
    "info = pd.read_excel(data_path+'clinical_info.xlsx')\n",
    "\n",
    "df_train = pd.read_csv(data_path+'train.csv')\n",
    "df_test = pd.read_csv(data_path+'test.csv')\n",
    "sub = pd.read_csv(data_path+'sample_submission.csv')\n",
    "\n",
    "df_train['img_path'] = df_train['img_path'].apply(lambda x: x.replace('./', '../open/'))\n",
    "df_test['img_path'] = df_test['img_path'].apply(lambda x: x.replace('./', '../open/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_vectors = []\n",
    "test_img_vectors = []\n",
    "\n",
    "resize = A.Resize(224, 224)\n",
    "normalize = A.transforms.Normalize(\n",
    "                                    mean=(0.5, 0.5, 0.5), \n",
    "                                    std=(0.5, 0.5, 0.5), \n",
    "                                    max_pixel_value=255.0, \n",
    "                                    p=1.0)\n",
    "\n",
    "model = timm.create_model(model_name='tf_efficientnetv2_l_in21k', pretrained=True, in_chans=3)\n",
    "model.classifier = nn.Identity()\n",
    "# fc_layer = nn.Linear(1280, 128)\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "for img_path in tqdm(df_train['img_path']):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = normalize(image=resize(image=img)['image'])['image']\n",
    "    img = ToTensorV2()(image=img)['image'].unsqueeze(0).to('cuda')\n",
    "    \n",
    "    # img_vector = fc_layer(model(img)).to('mps')\n",
    "    img_vector = model(img).squeeze().cpu().detach().numpy()\n",
    "\n",
    "    train_img_vectors.append(img_vector)\n",
    "\n",
    "for img_path in tqdm(df_test['img_path']):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = normalize(image=resize(image=img)['image'])['image']\n",
    "    img = ToTensorV2()(image=img)['image'].unsqueeze(0).to('cuda')\n",
    "    \n",
    "    # img_vector = fc_layer(model(img)).to('mps')\n",
    "    img_vector = model(img).squeeze().cpu().detach().numpy()\n",
    "\n",
    "    test_img_vectors.append(img_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_df = pd.DataFrame(np.array(train_img_vectors), \n",
    "                    columns=['img_rep_' + str(i+1) for i in range(train_img_vectors[0].shape[0])])\n",
    "test_img_df = pd.DataFrame(np.array(test_img_vectors), \n",
    "                    columns=['img_rep_' + str(i+1) for i in range(train_img_vectors[0].shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_df.to_csv('../open/train_img_vector.csv', index=False)\n",
    "test_img_df.to_csv('../open/test_img_vector.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fold : 0\n",
      "start fold : 1\n",
      "start fold : 2\n",
      "start fold : 3\n",
      "start fold : 4\n"
     ]
    }
   ],
   "source": [
    "data_path = '../open/'\n",
    "df_train = pd.read_csv(data_path+'train.csv')\n",
    "df_test = pd.read_csv(data_path+'test.csv')\n",
    "\n",
    "train_img_df = pd.read_csv(data_path+'train_img_vector.csv')\n",
    "test_img_df = pd.read_csv(data_path+'test_img_vector.csv')\n",
    "\n",
    "perm_res = []\n",
    "\n",
    "df_train['fe1'] = df_train['암의 장경'] / df_train['암의 개수'].fillna(1)\n",
    "df_test['fe1'] = df_test['암의 장경'] / df_test['암의 개수'].fillna(1)\n",
    "\n",
    "\n",
    "n_components = 10\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "train_pca = pca.fit_transform(train_img_df.values)\n",
    "test_pca = pca.transform(test_img_df.values)\n",
    "df_train = pd.concat([df_train, \n",
    "                pd.DataFrame(train_pca, columns=['pca_'+str(i+1) for i in range(n_components)])], axis=1)\n",
    "df_test = pd.concat([df_test, \n",
    "                pd.DataFrame(test_pca, columns=['pca_'+str(i+1) for i in range(n_components)])], axis=1)\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    df['암의 장경'] = pd.cut(df['암의 장경'], [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200])\n",
    "    df['fe1'] = pd.cut(df['fe1'], [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200])\n",
    "\n",
    "FOLDS = 5\n",
    "\n",
    "def basic_set():\n",
    "    use_features = [\n",
    "                    '나이', \n",
    "                    '진단명', '암의 위치', '암의 개수', '암의 장경', 'NG', 'HG', 'HG_score_1', 'HG_score_2', 'HG_score_3',\n",
    "                    'DCIS_or_LCIS_여부', 'DCIS_or_LCIS_type', 'T_category', \n",
    "                    'ER', 'ER_Allred_score', 'PR', 'PR_Allred_score', \n",
    "                    'KI-67_LI_percent', \n",
    "                    'HER2', 'HER2_IHC', 'HER2_SISH', 'HER2_SISH_ratio', 'BRCA_mutation', \n",
    "                    'fe1', \n",
    "                    ]\n",
    "    use_features += ['pca_'+str(i+1) for i in range(n_components)]\n",
    "\n",
    "    cat_features = ['암의 장경', 'fe1']\n",
    "    return use_features, cat_features\n",
    "\n",
    "df_train['due_date'] = 2022 - pd.to_datetime(df_train['수술연월일']).dt.year\n",
    "df_test['due_date'] = 2022 - pd.to_datetime(df_test['수술연월일']).dt.year\n",
    "\n",
    "df_train['date_year'] = pd.to_datetime(df_train['수술연월일']).dt.year\n",
    "df_test['date_year'] = pd.to_datetime(df_test['수술연월일']).dt.year\n",
    "\n",
    "for col in [\n",
    "                '나이', '진단명', '암의 위치', '암의 개수', 'NG', 'HG', 'HG_score_1', 'HG_score_2', 'HG_score_3',\n",
    "                'DCIS_or_LCIS_여부', 'DCIS_or_LCIS_type', 'T_category', \n",
    "                'ER', 'ER_Allred_score', 'PR', 'PR_Allred_score', \n",
    "                'HER2', 'HER2_IHC', 'HER2_SISH', 'HER2_SISH_ratio', 'BRCA_mutation', 'due_date', 'date_year',\n",
    "                # 'fe1',\n",
    "                ]:\n",
    "    df_test[col] = df_test[col].map(df_train[col].value_counts(True))\n",
    "    df_train[col] = df_train[col].map(df_train[col].value_counts(True))\n",
    "    \n",
    "skf = StratifiedKFold(n_splits=FOLDS, random_state=42, shuffle=True)\n",
    "splits = list(skf.split(df_train, df_train['N_category']))\n",
    "\n",
    "\n",
    "fi_df = pd.DataFrame()\n",
    "oof = np.zeros(len(df_train))\n",
    "preds = np.zeros(len(df_test))\n",
    "preds_arr = np.zeros([len(df_test), 5])\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    print('start fold :', fold)\n",
    "    test = df_test.copy()\n",
    "    \n",
    "    use_features, cat_features = basic_set()\n",
    "    use_features += ['due_date', 'date_year']\n",
    "\n",
    "    tt = df_train.loc[splits[fold][0], use_features].reset_index(drop=True)\n",
    "    tt_target = df_train.loc[splits[fold][0], 'N_category'].values\n",
    "    vv = df_train.loc[splits[fold][1], use_features].reset_index(drop=True)\n",
    "    vv_target = df_train.loc[splits[fold][1], 'N_category'].values\n",
    "\n",
    "    for col in ['암의 장경', 'fe1']:\n",
    "        tmp_dict = {j:i for i,j in enumerate(np.unique(df_train[col].astype(str)))}\n",
    "        tt[col] = tt[col].astype(str).map(tmp_dict).astype(int)\n",
    "        vv[col] = vv[col].astype(str).map(tmp_dict).astype(int)\n",
    "        test[col] = test[col].astype(str).map(tmp_dict).astype(int)\n",
    "\n",
    "    # ctgan = pd.read_csv(f'../open/ctgan_csv/40/200/{fold}.csv')\n",
    "    # tt = pd.concat([tt, ctgan[use_features]], ignore_index=False)\n",
    "    # tt_target = np.concatenate([tt_target, ctgan['target'].values])\n",
    "\n",
    "    # df = pd.concat([tt, pd.DataFrame(tt_target, columns=['target'])], axis=1)\n",
    "\n",
    "    # ctgan = CTGAN(epochs=20)\n",
    "    # ctgan.fit(df)\n",
    "    # # Create synthetic data\n",
    "    # synthetic_data = ctgan.sample(500)\n",
    "    # df = pd.concat([df, synthetic_data], ignore_index=True)\n",
    "    # tt_target = df['target'].values\n",
    "    # tt = df.drop(columns='target')\n",
    "    \n",
    "    test = test[use_features].reset_index(drop=True)\n",
    "\n",
    "    # lgb = LGBMClassifier(\n",
    "    #                     n_estimators=5000,\n",
    "    #                     learning_rate=0.03,\n",
    "    #                     max_depth=-1,\n",
    "    #                     num_leaves=64,\n",
    "    #                     )\n",
    "\n",
    "    # lgb.fit(tt, tt_target, eval_set=[(tt, tt_target), (vv, vv_target)], verbose=1000, early_stopping_rounds=500)\n",
    "    # oof[splits[fold][1]] += lgb.predict_proba(vv)[:, 1]\n",
    "    # preds += lgb.predict_proba(test)[:, 1] / FOLDS\n",
    "    # fi_df = pd.concat([fi_df, pd.DataFrame(zip(use_features, lgb.feature_importances_), columns=['feature', 'value'])])\n",
    "    # tt[cat_features] = tt[cat_features].fillna(-99).astype(int)\n",
    "    # vv[cat_features] = vv[cat_features].fillna(-99).astype(int)\n",
    "    \n",
    "    # lr = LogisticRegression(max_iter=1000)\n",
    "    # lr.fit(tt.fillna(0), tt_target)\n",
    "    # tt['lr_preds'] = lr.predict_proba(tt.fillna(0))[:, 1]\n",
    "    # vv['lr_preds'] = lr.predict_proba(vv.fillna(0))[:, 1]\n",
    "    # test['lr_preds'] = lr.predict_proba(test.fillna(0))[:, 1]\n",
    "    # tt = tt.drop(columns=perm_rm_features[fold])\n",
    "    # vv = vv.drop(columns=perm_rm_features[fold])\n",
    "    # test = test.drop(columns=perm_rm_features[fold])\n",
    "    \n",
    "    cat = CatBoostClassifier(\n",
    "                            n_estimators=5000,\n",
    "                            learning_rate=0.03,\n",
    "                            # border_count=4,\n",
    "                            # depth=4,\n",
    "                            # task_type=\"GPU\",\n",
    "                            # devices='0',\n",
    "                            # one_hot_max_size=2,\n",
    "                            # objective='logloss',\n",
    "                            # eval_metric='F1',\n",
    "                            # reg_lambda=0.01,\n",
    "                            # random_seed=1028,\n",
    "                            allow_writing_files=False,\n",
    "                            logging_level='Silent',\n",
    "                            )\n",
    "    cat.fit(tt, tt_target, eval_set=[(vv, vv_target)], verbose=1000, early_stopping_rounds=500,\n",
    "            # cat_features=cat_features, \n",
    "            )\n",
    "    # cat.fit(tt[lambda x: x['due_date']<14], tt_target[tt['due_date']<14], eval_set=[(tt, tt_target), (vv, vv_target)], verbose=1000, early_stopping_rounds=500,\n",
    "    #         # cat_features=cat_features\n",
    "    #         )\n",
    "    \n",
    "    oof[splits[fold][1]] += cat.predict_proba(vv)[:, 1]\n",
    "    preds += cat.predict_proba(test)[:, 1] / FOLDS\n",
    "    preds_arr[:, fold] = cat.predict_proba(test)[:, 1]\n",
    "    fi_df = pd.concat([fi_df, pd.DataFrame(zip(use_features, cat.feature_importances_), columns=['feature', 'value'])])\n",
    "\n",
    "    # res = permutation_importance(cat, vv, vv_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.838999838999839"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no vc : 0.8289984609861489\n",
    "# vc : 0.8369867959304704\n",
    "# pca10 : 0.838999838999839\n",
    "f1_score(df_train['N_category'],\n",
    "        np.where(oof>0.5, 1, 0),\n",
    "        average='macro'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = pd.read_csv('../submit/catboost_ctgan60e_600s.csv')\n",
    "c = pd.Series(np.where(preds>0.5, 1, 0))\n",
    "\n",
    "\n",
    "np.not_equal(c, b['N_category']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>date_year</td>\n",
       "      <td>10.816105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>due_date</td>\n",
       "      <td>10.323797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NG</td>\n",
       "      <td>8.008914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HG_score_2</td>\n",
       "      <td>6.989409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pca_2</td>\n",
       "      <td>5.950414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DCIS_or_LCIS_여부</td>\n",
       "      <td>5.225440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pca_1</td>\n",
       "      <td>4.426856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>KI-67_LI_percent</td>\n",
       "      <td>4.150771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>암의 장경</td>\n",
       "      <td>3.963540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>나이</td>\n",
       "      <td>3.751106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HG_score_1</td>\n",
       "      <td>3.667069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>암의 개수</td>\n",
       "      <td>3.663151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>암의 위치</td>\n",
       "      <td>3.174947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>fe1</td>\n",
       "      <td>2.555539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>진단명</td>\n",
       "      <td>2.372081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PR_Allred_score</td>\n",
       "      <td>2.304024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HER2_IHC</td>\n",
       "      <td>2.259820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DCIS_or_LCIS_type</td>\n",
       "      <td>2.110481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HG</td>\n",
       "      <td>2.064220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HG_score_3</td>\n",
       "      <td>2.042789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PR</td>\n",
       "      <td>1.883869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>T_category</td>\n",
       "      <td>1.650984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ER_Allred_score</td>\n",
       "      <td>1.599661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HER2_SISH_ratio</td>\n",
       "      <td>1.457858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HER2</td>\n",
       "      <td>1.339229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ER</td>\n",
       "      <td>1.060581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HER2_SISH</td>\n",
       "      <td>0.842755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BRCA_mutation</td>\n",
       "      <td>0.344587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature      value\n",
       "18          date_year  10.816105\n",
       "19           due_date  10.323797\n",
       "14                 NG   8.008914\n",
       "11         HG_score_2   6.989409\n",
       "22              pca_2   5.950414\n",
       "2     DCIS_or_LCIS_여부   5.225440\n",
       "21              pca_1   4.426856\n",
       "13   KI-67_LI_percent   4.150771\n",
       "26              암의 장경   3.963540\n",
       "23                 나이   3.751106\n",
       "10         HG_score_1   3.667069\n",
       "24              암의 개수   3.663151\n",
       "25              암의 위치   3.174947\n",
       "20                fe1   2.555539\n",
       "27                진단명   2.372081\n",
       "16    PR_Allred_score   2.304024\n",
       "6            HER2_IHC   2.259820\n",
       "1   DCIS_or_LCIS_type   2.110481\n",
       "9                  HG   2.064220\n",
       "12         HG_score_3   2.042789\n",
       "15                 PR   1.883869\n",
       "17         T_category   1.650984\n",
       "4     ER_Allred_score   1.599661\n",
       "8     HER2_SISH_ratio   1.457858\n",
       "5                HER2   1.339229\n",
       "3                  ER   1.060581\n",
       "7           HER2_SISH   0.842755\n",
       "0       BRCA_mutation   0.344587"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi_df.groupby('feature').mean().reset_index().sort_values('value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    0.523\n",
       " 1    0.477\n",
       " dtype: float64,\n",
       " 1    0.514\n",
       " 0    0.486\n",
       " Name: N_category, dtype: float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.where(oof>0.5, 1, 0)).value_counts(True), df_train['N_category'].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1    0.5\n",
       " 0    0.5\n",
       " dtype: float64,\n",
       " 0    0.54\n",
       " 1    0.46\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.where(preds>0.4, 1, 0)).value_counts(True), pd.Series(np.where(preds>0.5, 1, 0)).value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(data_path+'sample_submission.csv')\n",
    "sub['N_category'] = np.where(preds>=0.5, 1, 0)\n",
    "\n",
    "sub.to_csv('../submit/catboost_img_pca_vc__ctgan40e_200s.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../open/'\n",
    "df_train = pd.read_csv(data_path+'train.csv')\n",
    "df_test = pd.read_csv(data_path+'test.csv')\n",
    "perm_res = []\n",
    "\n",
    "df_train['fe1'] = df_train['암의 장경'] / df_train['암의 개수'].fillna(1)\n",
    "df_test['fe1'] = df_test['암의 장경'] / df_test['암의 개수'].fillna(1)\n",
    "\n",
    "\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "train_pca = pca.fit_transform(train_img_df.values)\n",
    "test_pca = pca.transform(test_img_df.values)\n",
    "df_train = pd.concat([df_train, \n",
    "                pd.DataFrame(train_pca, columns=['pca_'+str(i+1) for i in range(n_components)])], axis=1)\n",
    "df_test = pd.concat([df_test, \n",
    "                pd.DataFrame(test_pca, columns=['pca_'+str(i+1) for i in range(n_components)])], axis=1)\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    df['암의 장경'] = pd.cut(df['암의 장경'], [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200])\n",
    "    df['fe1'] = pd.cut(df['fe1'], [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200])\n",
    "\n",
    "FOLDS = 5\n",
    "\n",
    "def basic_set():\n",
    "    use_features = [\n",
    "                    '나이', \n",
    "                    '진단명', '암의 위치', '암의 개수', '암의 장경', 'NG', 'HG', 'HG_score_1', 'HG_score_2', 'HG_score_3',\n",
    "                    'DCIS_or_LCIS_여부', 'DCIS_or_LCIS_type', 'T_category', \n",
    "                    'ER', 'ER_Allred_score', 'PR', 'PR_Allred_score', \n",
    "                    'KI-67_LI_percent', \n",
    "                    'HER2', 'HER2_IHC', 'HER2_SISH', 'HER2_SISH_ratio', 'BRCA_mutation', \n",
    "                    'fe1', \n",
    "                    ]\n",
    "    use_features += ['pca_'+str(i+1) for i in range(n_components)]\n",
    "\n",
    "    cat_features = ['암의 장경', 'fe1']\n",
    "    return use_features, cat_features\n",
    "\n",
    "df_train['due_date'] = 2022 - pd.to_datetime(df_train['수술연월일']).dt.year\n",
    "df_test['due_date'] = 2022 - pd.to_datetime(df_test['수술연월일']).dt.year\n",
    "\n",
    "df_train['date_year'] = pd.to_datetime(df_train['수술연월일']).dt.year\n",
    "df_test['date_year'] = pd.to_datetime(df_test['수술연월일']).dt.year\n",
    "\n",
    "skf = StratifiedKFold(n_splits=FOLDS, random_state=42, shuffle=True)\n",
    "splits = list(skf.split(df_train, df_train['N_category']))\n",
    "\n",
    "\n",
    "fi_df = pd.DataFrame()\n",
    "oof = np.zeros(len(df_train))\n",
    "preds = np.zeros(len(df_test))\n",
    "preds_arr = np.zeros([len(df_test), 5])\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    print('start fold :', fold)\n",
    "    test = df_test.copy()\n",
    "    \n",
    "    use_features, cat_features = basic_set()\n",
    "    use_features += ['due_date', 'date_year']\n",
    "    # use_features += ['due_date', 'date_year', '암의 장경/개수']\n",
    "\n",
    "    tt = df_train.loc[splits[fold][0], use_features].reset_index(drop=True)\n",
    "    tt_target = df_train.loc[splits[fold][0], 'N_category'].values\n",
    "    vv = df_train.loc[splits[fold][1], use_features].reset_index(drop=True)\n",
    "    vv_target = df_train.loc[splits[fold][1], 'N_category'].values\n",
    "\n",
    "    for col in ['암의 장경', 'fe1']:\n",
    "        tmp_dict = {j:i for i,j in enumerate(np.unique(df_train[col].astype(str)))}\n",
    "        tt[col] = tt[col].astype(str).map(tmp_dict).astype(int)\n",
    "        vv[col] = vv[col].astype(str).map(tmp_dict).astype(int)\n",
    "        test[col] = test[col].astype(str).map(tmp_dict).astype(int)\n",
    "\n",
    "    # ctgan = pd.read_csv(f'../open/ctgan_csv/50/400/{fold}.csv')\n",
    "    # tt = pd.concat([tt, ctgan[use_features]], ignore_index=False)\n",
    "    # tt_target = np.concatenate([tt_target, ctgan['target'].values])\n",
    "\n",
    "    df = pd.concat([tt, pd.DataFrame(tt_target, columns=['target'])], axis=1)\n",
    "\n",
    "    ctgan = CTGAN(epochs=20)\n",
    "    ctgan.fit(df)\n",
    "    # Create synthetic data\n",
    "    synthetic_data = ctgan.sample(500)\n",
    "    df = pd.concat([df, synthetic_data], ignore_index=True)\n",
    "    tt_target = df['target'].values\n",
    "    tt = df.drop(columns='target')\n",
    "    \n",
    "    test = test[use_features].reset_index(drop=True)\n",
    "\n",
    "    # lgb = LGBMClassifier(\n",
    "    #                     n_estimators=5000,\n",
    "    #                     learning_rate=0.03,\n",
    "    #                     max_depth=-1,\n",
    "    #                     num_leaves=64,\n",
    "    #                     )\n",
    "\n",
    "    # lgb.fit(tt, tt_target, eval_set=[(tt, tt_target), (vv, vv_target)], verbose=1000, early_stopping_rounds=500)\n",
    "    # oof[splits[fold][1]] += lgb.predict_proba(vv)[:, 1]\n",
    "    # preds += lgb.predict_proba(test)[:, 1] / FOLDS\n",
    "    # fi_df = pd.concat([fi_df, pd.DataFrame(zip(use_features, lgb.feature_importances_), columns=['feature', 'value'])])\n",
    "    # tt[cat_features] = tt[cat_features].fillna(-99).astype(int)\n",
    "    # vv[cat_features] = vv[cat_features].fillna(-99).astype(int)\n",
    "    \n",
    "    # lr = LogisticRegression(max_iter=1000)\n",
    "    # lr.fit(tt.fillna(0), tt_target)\n",
    "    # tt['lr_preds'] = lr.predict_proba(tt.fillna(0))[:, 1]\n",
    "    # vv['lr_preds'] = lr.predict_proba(vv.fillna(0))[:, 1]\n",
    "    # test['lr_preds'] = lr.predict_proba(test.fillna(0))[:, 1]\n",
    "    # tt = tt.drop(columns=perm_rm_features[fold])\n",
    "    # vv = vv.drop(columns=perm_rm_features[fold])\n",
    "    # test = test.drop(columns=perm_rm_features[fold])\n",
    "    \n",
    "    cat = CatBoostClassifier(\n",
    "                            n_estimators=5000,\n",
    "                            learning_rate=0.03,\n",
    "                            # border_count=4,\n",
    "                            # depth=4,\n",
    "                            # task_type=\"GPU\",\n",
    "                            # devices='0',\n",
    "                            # one_hot_max_size=2,\n",
    "                            # objective='logloss',\n",
    "                            # eval_metric='F1',\n",
    "                            # reg_lambda=0.01,\n",
    "                            # random_seed=1028,\n",
    "                            allow_writing_files=False,\n",
    "                            logging_level='Silent',\n",
    "                            )\n",
    "    cat.fit(tt, tt_target, eval_set=[(vv, vv_target)], verbose=1000, early_stopping_rounds=500,\n",
    "            # cat_features=cat_features, \n",
    "            )\n",
    "    # cat.fit(tt[lambda x: x['due_date']<14], tt_target[tt['due_date']<14], eval_set=[(tt, tt_target), (vv, vv_target)], verbose=1000, early_stopping_rounds=500,\n",
    "    #         # cat_features=cat_features\n",
    "    #         )\n",
    "    \n",
    "    oof[splits[fold][1]] += cat.predict_proba(vv)[:, 1]\n",
    "    preds += cat.predict_proba(test)[:, 1] / FOLDS\n",
    "    preds_arr[:, fold] = cat.predict_proba(test)[:, 1]\n",
    "    fi_df = pd.concat([fi_df, pd.DataFrame(zip(use_features, cat.feature_importances_), columns=['feature', 'value'])])\n",
    "\n",
    "    # res = permutation_importance(cat, vv, vv_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8449961249031226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(df_train['N_category'],\n",
    "        np.where(oof>0.5, 1, 0),\n",
    "        average='macro'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(df_train['N_category'],\n",
    "        np.where(oof>0.5, 1, 0),\n",
    "        average='macro'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.read_csv('../submit/catboost_ctgan60e_600s.csv')\n",
    "c = pd.Series(np.where(preds>0.45, 1, 0))\n",
    "\n",
    "\n",
    "np.not_equal(c, b['N_category']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(df_train['N_category'],\n",
    "        np.where(oof>0.5, 1, 0),\n",
    "        average='macro'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_df.groupby('feature').mean().reset_index().sort_values('value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.where(oof>0.5, 1, 0)).value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['N_category'].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.where(preds>0.4, 1, 0)).value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.where(preds>0.5, 1, 0)).value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['N_category'] = np.where(preds>=0.45, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../submit/catboost__ctgan60e_800s__round45.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../submit/cat_oof.npy', oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../submit/cat_preds.npy', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(data_path+'df_train.csv')\n",
    "df_test = pd.read_csv(data_path+'df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['due_date'] = 2022 - pd.to_datetime(df_train['수술연월일']).dt.year\n",
    "df_test['due_date'] = 2022 - pd.to_datetime(df_test['수술연월일']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['암의 위치']df_train['암의 개수'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_oof = []\n",
    "seed_pred = []\n",
    "for seed in [1028, 42, 204, 1510, 99]:\n",
    "\n",
    "    df_train = pd.read_csv(data_path+'train.csv')\n",
    "    df_test = pd.read_csv(data_path+'test.csv')\n",
    "\n",
    "    # preprocess outlier\n",
    "    # df_train['PR_Allred_score'] = df_train['PR_Allred_score'].where((0<=df_train['PR_Allred_score']) & (df_train['PR_Allred_score']<=8))\n",
    "\n",
    "    # for col in ['NG', 'HG', 'HG_score_1', 'HG_score_2', 'HG_score_3', 'DCIS_or_LCIS_type', 'ER_Allred_score', 'PR_Allred_score', 'HER2_SISH_ratio']:\n",
    "    #     df_train[col].fillna(0, inplace=True)\n",
    "    #     df_test[col].fillna(0, inplace=True)\n",
    "        # df_train[col].fillna(-1, inplace=True)\n",
    "        # df_test[col].fillna(-1, inplace=True)\n",
    "\n",
    "    # df_test['암의 장경'].fillna(df_train['암의 장경'].median(), inplace=True)\n",
    "    # df_train['암의 장경'].fillna(df_train['암의 장경'].median(), inplace=True)\n",
    "\n",
    "    # df_train['BRCA_mutation'] = df_train['BRCA_mutation'].fillna(1)\n",
    "    # df_test['BRCA_mutation'] = df_test['BRCA_mutation'].fillna(1)\n",
    "\n",
    "    # for col in ['T_category', 'HER2', 'HER2_IHC', 'HER2_SISH', 'KI-67_LI_percent']:\n",
    "    #     df_train[col].fillna(-1, inplace=True)\n",
    "    #     df_test[col].fillna(-1, inplace=True)\n",
    "\n",
    "    #     df_train[col]+=1\n",
    "    #     df_test[col]+=1\n",
    "\n",
    "    # # remove ER, PR nan value\n",
    "    # df_train = df_train.drop(266).reset_index(drop=True)\n",
    "\n",
    "    FOLDS = 5\n",
    "\n",
    "    def basic_set():\n",
    "        use_features = [\n",
    "                        '나이', \n",
    "                        '진단명', '암의 위치', '암의 개수', '암의 장경', 'NG', 'HG', 'HG_score_1', 'HG_score_2', 'HG_score_3',\n",
    "                        'DCIS_or_LCIS_여부', 'DCIS_or_LCIS_type', 'T_category', \n",
    "                        'ER', 'ER_Allred_score', 'PR', 'PR_Allred_score', \n",
    "                        'KI-67_LI_percent', \n",
    "                        'HER2', 'HER2_IHC', 'HER2_SISH', 'HER2_SISH_ratio', 'BRCA_mutation', \n",
    "                        # 'img_height', 'img_width',\n",
    "                        # 'BRCA_mutation2',\n",
    "                        ]\n",
    "\n",
    "        cat_features = ['나이', '진단명', '암의 위치', '암의 개수', '암의 장경', 'NG', 'HG', 'HG_score_1',\n",
    "                                'HG_score_2', 'HG_score_3', 'DCIS_or_LCIS_여부', 'DCIS_or_LCIS_type',\n",
    "                                'T_category', 'ER', 'ER_Allred_score', 'PR', 'PR_Allred_score',\n",
    "                                'HER2', 'HER2_IHC', 'HER2_SISH','BRCA_mutation']\n",
    "        cat_features = ['진단명']\n",
    "        return use_features, cat_features\n",
    "\n",
    "    df_train['due_date'] = 2022 - pd.to_datetime(df_train['수술연월일']).dt.year\n",
    "    df_test['due_date'] = 2022 - pd.to_datetime(df_test['수술연월일']).dt.year\n",
    "    # df_train['due_date2'] = (2022-pd.to_datetime(df_train['수술연월일']).dt.year)*12 + pd.to_datetime(df_train['수술연월일']).dt.month\n",
    "    # df_test['due_date2'] = (2022-pd.to_datetime(df_test['수술연월일']).dt.year)*12 + pd.to_datetime(df_test['수술연월일']).dt.month\n",
    "\n",
    "    df_train['date_year'] = pd.to_datetime(df_train['수술연월일']).dt.year\n",
    "    df_test['date_year'] = pd.to_datetime(df_test['수술연월일']).dt.year\n",
    "\n",
    "    # df_train['수술당시나이'] = df_train['나이'] - df_train['due_date']\n",
    "    # df_test['수술당시나이'] = df_test['나이'] - df_test['due_date']\n",
    "\n",
    "    # df_train['암의 장경/개수'] = df_train['암의 장경'] / df_train['암의 개수']\n",
    "    # df_test['암의 장경/개수'] = df_test['암의 장경'] / df_test['암의 개수']\n",
    "\n",
    "    for col in [\n",
    "                        '나이', '진단명', '암의 위치', '암의 개수', 'NG', 'HG', 'HG_score_1', 'HG_score_2', 'HG_score_3',\n",
    "                        'DCIS_or_LCIS_여부', 'DCIS_or_LCIS_type', 'T_category', \n",
    "                        'ER', 'ER_Allred_score', 'PR', 'PR_Allred_score', \n",
    "                        'HER2', 'HER2_IHC', 'HER2_SISH', 'HER2_SISH_ratio', 'BRCA_mutation', 'due_date', 'date_year']:\n",
    "        df_test[col] = df_test[col].map(df_train[col].value_counts(True))\n",
    "        df_train[col] = df_train[col].map(df_train[col].value_counts(True))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, random_state=seed, shuffle=True)\n",
    "    splits = list(skf.split(df_train, df_train['N_category']))\n",
    "    # labels = df_train['N_category'].astype(str) + '_' + df_train['due_date'].astype(str)\n",
    "    # splits = list(skf.split(df_train, labels))\n",
    "\n",
    "\n",
    "    fi_df = pd.DataFrame()\n",
    "    oof = np.zeros(len(df_train))\n",
    "    preds = np.zeros(len(df_test))\n",
    "\n",
    "    for fold in range(FOLDS):\n",
    "        print('start fold :', fold)\n",
    "        test = df_test.copy()\n",
    "        \n",
    "        use_features, cat_features = basic_set()\n",
    "        use_features += ['due_date', 'date_year']\n",
    "        # use_features += ['due_date', 'date_year', '암의 장경/개수']\n",
    "\n",
    "        tt = df_train.loc[splits[fold][0], use_features].reset_index(drop=True)\n",
    "        tt_target = df_train.loc[splits[fold][0], 'N_category'].values\n",
    "        vv = df_train.loc[splits[fold][1], use_features].reset_index(drop=True)\n",
    "        vv_target = df_train.loc[splits[fold][1], 'N_category'].values\n",
    "\n",
    "        for df in [tt, vv, test]:\n",
    "            # df['나이'] = pd.cut(df['나이'], [20, 30, 40, 50, 60, 70, 80, 100])\n",
    "            df['암의 장경'] = pd.cut(df['암의 장경'], [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200])\n",
    "\n",
    "            # df['due_date'] = pd.cut(df['due_date'], [0, 5, 10, 15])\n",
    "            # df['date_year'] = pd.cut(df['date_year'], [0, 2005, 2010, 2015, 2020, 2030])\n",
    "            # df['암의 장경'] = pd.cut(df['암의 장경'], [0, 20, 40, 60, 80, 100, 200])\n",
    "            # df['암의 장경/개수'] = pd.cut(df['암의 장경/개수'], [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200])\n",
    "            # df['KI-67_LI_percent'] = pd.cut(df['KI-67_LI_percent'], [0, 20, 40, 60, 80, 100])\n",
    "            \n",
    "        # for col in ['나이', '암의 장경', '암의 장경/개수']:\n",
    "        # for col in ['나이', '암의 장경', 'KI-67_LI_percent']:\n",
    "        # for col in ['나이', '암의 장경']:\n",
    "        for col in ['암의 장경']:\n",
    "            tmp_dict = {j:i for i,j in enumerate(np.unique(tt[col].astype(str)))}\n",
    "            tt[col] = tt[col].astype(str).map(tmp_dict)\n",
    "            vv[col] = vv[col].astype(str).map(tmp_dict)\n",
    "            test[col] = test[col].astype(str).map(tmp_dict)\n",
    "\n",
    "        # generate dataset\n",
    "        # tt = tt.fillna(0)\n",
    "        # vv = vv.fillna(0)\n",
    "        # test = test.fillna(0)\n",
    "\n",
    "        # df = pd.concat([tt, pd.DataFrame(tt_target, columns=['target'])], axis=1)\n",
    "\n",
    "        # # discrete_columns = ['암의 장경', 'target']\n",
    "\n",
    "        # # ctgan = CTGAN(epochs=10)\n",
    "        # ctgan = CopulaGAN(epochs=10)\n",
    "        # ctgan.fit(df)\n",
    "        # # Create synthetic data\n",
    "        # synthetic_data = ctgan.sample(50, randomize_samples=False)\n",
    "        # df = pd.concat([df, synthetic_data], ignore_index=True)\n",
    "        # tt_target = df['target'].values\n",
    "        # tt = df.drop(columns='target')\n",
    "        ctgan = pd.read_csv(f'../open/ctgan_csv/100/500/{fold}.csv')\n",
    "        tt = pd.concat([tt, ctgan[use_features]], ignore_index=False)\n",
    "        tt_target = np.concatenate([tt_target, ctgan['target'].values])\n",
    "        \n",
    "        # tt['암의 장경2'] = tt['암의 장경'].isnull()\n",
    "        # vv['암의 장경2'] = vv['암의 장경'].isnull()\n",
    "        # test['암의 장경2'] = test['암의 장경'].isnull()\n",
    "\n",
    "        # for df in [tt, vv, test]:\n",
    "        #     # df['나이_due_date'] = df['나이'].astype(str) + '_' + df['due_date'].astype(str)\n",
    "        #     df['암'] = df['암의 위치'].astype(str) + '_' + df['암의 장경'].astype(str)\n",
    "        # df = pd.concat([tt, vv])\n",
    "        # tmp_dict = {v:i for i, v in enumerate(np.unique(df['암']))}\n",
    "        # for col in ['암']:\n",
    "        #     tt[col] = tt[col].astype(str).map(tmp_dict)\n",
    "        #     vv[col] = vv[col].astype(str).map(tmp_dict)\n",
    "        #     test[col] = test[col].astype(str).map(tmp_dict)\n",
    "        \n",
    "        # use_features += ['암']\n",
    "        \n",
    "        # tmp_dict = df_train.loc[splits[fold][0]].groupby('due_date')['N_category'].mean()\n",
    "        # tt['진단명_target'] = tt['due_date'].map(tmp_dict)\n",
    "        # vv['진단명_target'] = vv['due_date'].map(tmp_dict)\n",
    "        # tmp_dict = df_train.groupby('due_date')['N_category'].mean()\n",
    "        # test['진단명_target'] = test['due_date'].map(tmp_dict)\n",
    "        # use_features += ['진단명_target']\n",
    "        \n",
    "\n",
    "\n",
    "        test = test[use_features].reset_index(drop=True)\n",
    "\n",
    "        # lgb = LGBMClassifier(\n",
    "        #                     n_estimators=5000,\n",
    "        #                     learning_rate=0.03,\n",
    "        #                     max_depth=-1,\n",
    "        #                     num_leaves=64,\n",
    "        #                     )\n",
    "\n",
    "        # lgb.fit(tt, tt_target, eval_set=[(tt, tt_target), (vv, vv_target)], verbose=1000, early_stopping_rounds=100)\n",
    "        # oof[splits[fold][1]] += lgb.predict_proba(vv)[:, 1]\n",
    "        # preds += lgb.predict_proba(test)[:, 1] / 5\n",
    "        # fi_df = pd.concat([fi_df, pd.DataFrame(zip(use_features, lgb.feature_importances_), columns=['feature', 'value'])])\n",
    "        # tt[cat_features] = tt[cat_features].fillna(-99).astype(int)\n",
    "        # vv[cat_features] = vv[cat_features].fillna(-99).astype(int)\n",
    "        cat = CatBoostClassifier(\n",
    "                                n_estimators=5000,\n",
    "                                learning_rate=0.03,\n",
    "                                # objective='logloss',\n",
    "                                # eval_metric='F1',\n",
    "                                # reg_lambda=0.01,\n",
    "                                # random_seed=1028,\n",
    "                                allow_writing_files=False,\n",
    "                                logging_level='Silent',\n",
    "                                )\n",
    "        cat.fit(tt, tt_target, eval_set=[(tt, tt_target), (vv, vv_target)], verbose=1000, early_stopping_rounds=500,\n",
    "                # cat_features=cat_features\n",
    "                )\n",
    "        # cat.fit(tt[lambda x: x['due_date']<14], tt_target[tt['due_date']<14], eval_set=[(tt, tt_target), (vv, vv_target)], verbose=1000, early_stopping_rounds=500,\n",
    "        #         # cat_features=cat_features\n",
    "        #         )\n",
    "\n",
    "                \n",
    "        oof[splits[fold][1]] += cat.predict_proba(vv)[:, 1]\n",
    "        preds += cat.predict_proba(test)[:, 1] / FOLDS\n",
    "        fi_df = pd.concat([fi_df, pd.DataFrame(zip(use_features, cat.feature_importances_), columns=['feature', 'value'])])\n",
    "        \n",
    "        \n",
    "    seed_oof.append(oof)\n",
    "    seed_pred.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(df_train['N_category'],\n",
    "         np.where(np.mean(seed_oof, 0)>0.5, 1, 0),\n",
    "         average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.read_csv('../submit/catboost_ctgan60e_600s.csv')\n",
    "c = pd.Series(np.where(np.mean(seed_pred, 0)>=0.5, 1, 0))\n",
    "\n",
    "\n",
    "np.not_equal(c, b['N_category']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['N_category'] = np.where(np.mean(seed_pred, 0)>=0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../submit/catboost_ctgan100e_500s__5seed__F1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ray_pl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65e69a851bb5a179bf7435f637a9dd3f2d67ff7b0f73f2181633d998f9e977c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
