{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import time\n",
    "import yaml\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from tabulate import tabulate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "\n",
    "from src import BC_Dataset, train_get_transforms, valid_get_transforms\n",
    "from src import pl_Wrapper\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "\n",
    "from src import obj, seed_everything, pl_Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../conf/base_config.yaml', encoding='utf-8') as f:\n",
    "    conf_yaml = yaml.safe_load(f)\n",
    "\n",
    "config = obj(conf_yaml)\n",
    "config.train_params.init_lr = float(config.train_params.init_lr)\n",
    "config.train_params.min_lr = float(config.train_params.min_lr)\n",
    "\n",
    "seed_everything(config.train_params.seed)\n",
    "\n",
    "df_train = pd.read_csv('../open/train.csv')\n",
    "df_test = pd.read_csv('../open/test.csv')\n",
    "sub = pd.read_csv('../open/sample_submission.csv')\n",
    "\n",
    "df_train['img_path'] = df_train['img_path'].apply(lambda x: x.replace('./', '../open/'))\n",
    "df_test['img_path'] = df_test['img_path'].apply(lambda x: x.replace('./', '../open/'))\n",
    "\n",
    "# preprocess outlier\n",
    "df_train['PR_Allred_score'] = df_train['PR_Allred_score'].where((0<=df_train['PR_Allred_score']) & (df_train['PR_Allred_score']<=8))\n",
    "\n",
    "for col in ['NG', 'HG', 'HG_score_1', 'HG_score_2', 'HG_score_3', 'DCIS_or_LCIS_type', 'ER_Allred_score', 'PR_Allred_score', 'HER2_SISH_ratio']:\n",
    "    df_train[col].fillna(0, inplace=True)\n",
    "    df_test[col].fillna(0, inplace=True)\n",
    "\n",
    "df_test['암의 장경'].fillna(df_train['암의 장경'].median(), inplace=True)\n",
    "df_train['암의 장경'].fillna(df_train['암의 장경'].median(), inplace=True)\n",
    "\n",
    "df_train['BRCA_mutation'] = df_train['BRCA_mutation'].fillna(1)\n",
    "df_test['BRCA_mutation'] = df_test['BRCA_mutation'].fillna(1)\n",
    "\n",
    "for col in ['T_category', 'HER2', 'HER2_IHC', 'HER2_SISH', 'KI-67_LI_percent']:\n",
    "    df_train[col].fillna(-1, inplace=True)\n",
    "    df_test[col].fillna(-1, inplace=True)\n",
    "\n",
    "    df_train[col]+=1\n",
    "    df_test[col]+=1\n",
    "\n",
    "df_train[['ER', 'PR']] = df_train[['ER', 'PR']].fillna(0)\n",
    "\n",
    "############## feature engineering\n",
    "df_train['due_date'] = 2022 - pd.to_datetime(df_train['수술연월일']).dt.year\n",
    "df_test['due_date'] = 2022 - pd.to_datetime(df_test['수술연월일']).dt.year\n",
    "\n",
    "df_train['date_year'] = pd.to_datetime(df_train['수술연월일']).dt.year\n",
    "df_test['date_year'] = pd.to_datetime(df_test['수술연월일']).dt.year\n",
    "\n",
    "config.train_params.numeric_features += ['due_date', 'date_year']\n",
    "\n",
    "for col in config.train_params.cat_features:\n",
    "    tmp_dict = {val:idx for idx, val in enumerate(np.unique(df_train[col]))}\n",
    "    df_train[col] = df_train[col].map(tmp_dict)\n",
    "    df_test[col] = df_test[col].map(tmp_dict)\n",
    "\n",
    "config.train_params.cat_features_ls = df_train[config.train_params.cat_features].nunique().values.tolist()\n",
    "config.train_params.num_numeric_features = len(config.train_params.numeric_features)\n",
    "config.embedding_size = 1024\n",
    "\n",
    "df_test['N_category'] = -1\n",
    "skf = StratifiedKFold(n_splits=config.train_params.folds, random_state=config.train_params.seed, shuffle=True)\n",
    "splits = list(skf.split(df_train, df_train['N_category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fold : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:49<00:00,  4.00it/s]\n",
      "100%|██████████| 250/250 [01:03<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load : ../models/tf_efficientnetv2_l_in21k_aug9/5fold_0__epoch=52_total_train_loss=0.86482_total_train_f1_score=0.77920_total_val_loss=1.28794_total_val_f1_score=0.81496.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]c:\\Users\\hhl\\Downloads\\cancer\\src\\..\\src\\data.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'img' : torch.tensor(img, dtype=torch.float32),\n",
      "2it [00:05,  2.56s/it]\n",
      "2it [00:03,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fold : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:49<00:00,  4.05it/s]\n",
      "100%|██████████| 250/250 [01:03<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load : ../models/tf_efficientnetv2_l_in21k_aug9/5fold_1__epoch=58_total_train_loss=1.65680_total_train_f1_score=0.70611_total_val_loss=1.14700_total_val_f1_score=0.80983.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.41s/it]\n",
      "2it [00:03,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fold : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:53<00:00,  3.76it/s]\n",
      "100%|██████████| 250/250 [01:05<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load : ../models/tf_efficientnetv2_l_in21k_aug9/5fold_2__epoch=71_total_train_loss=0.81006_total_train_f1_score=0.77656_total_val_loss=1.00243_total_val_f1_score=0.80496.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.44s/it]\n",
      "2it [00:03,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fold : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:50<00:00,  3.93it/s]\n",
      "100%|██████████| 250/250 [01:03<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load : ../models/tf_efficientnetv2_l_in21k_aug9/5fold_3__epoch=12_total_train_loss=1.31891_total_train_f1_score=0.73044_total_val_loss=1.37279_total_val_f1_score=0.80970.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.48s/it]\n",
      "2it [00:03,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fold : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:49<00:00,  4.06it/s]\n",
      "100%|██████████| 250/250 [01:03<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load : ../models/tf_efficientnetv2_l_in21k_aug9/5fold_4__epoch=30_total_train_loss=1.50568_total_train_f1_score=0.72945_total_val_loss=1.29398_total_val_f1_score=0.74960.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.47s/it]\n",
      "2it [00:03,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "valid_preds_fold = {}\n",
    "test_preds_fold = {}\n",
    "\n",
    "config.train_params.selected_folds = [0,1,2,3,4]\n",
    "if config.gpu.mps:\n",
    "    gpu = 'mps'\n",
    "else:\n",
    "    gpu = 'cuda'\n",
    "    \n",
    "for fold in config.train_params.selected_folds:\n",
    "    print('start fold :', fold)\n",
    "    config.start_time = time.strftime('%Y-%m-%d_%I:%M', time.localtime(time.time()))\n",
    "    \n",
    "    # tt = df_train.loc[splits[fold][0]].reset_index(drop=True)\n",
    "    vv = df_train.loc[splits[fold][1]].reset_index(drop=True)\n",
    "    valid_transforms = valid_get_transforms()\n",
    "\n",
    "    # config.train_dataset = BC_Dataset(tt, img_size=config.train_params.img_size, transform=train_transforms)\n",
    "    config.valid_dataset = BC_Dataset(config, vv, transform=valid_transforms)\n",
    "    config.test_dataset = BC_Dataset(config, df_test, transform=valid_transforms)\n",
    "    \n",
    "    model = pl_Wrapper(config).to(gpu)\n",
    "\n",
    "    fold_model_path = '../models/tf_efficientnetv2_l_in21k_aug9/'\n",
    "    model_path = sorted(os.listdir(fold_model_path))[fold]\n",
    "    model.load_state_dict(torch.load(fold_model_path + model_path)['state_dict'])\n",
    "    print('load :', fold_model_path + model_path)\n",
    "    \n",
    "    \n",
    "    valid_dataloader = DataLoader(\n",
    "                                config.valid_dataset,\n",
    "                                batch_size=128,\n",
    "                                num_workers=0,\n",
    "                                shuffle=False,\n",
    "                                sampler=SequentialSampler(config.valid_dataset),\n",
    "                                drop_last=False,\n",
    "                                pin_memory=True)\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "                                config.test_dataset,\n",
    "                                batch_size=128,\n",
    "                                num_workers=0,\n",
    "                                shuffle=False,\n",
    "                                sampler=SequentialSampler(config.test_dataset),\n",
    "                                drop_last=False,\n",
    "                                pin_memory=True)\n",
    "\n",
    "\n",
    "    valid_preds = []\n",
    "    valid_labels = []\n",
    "    test_preds = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in tqdm(enumerate(valid_dataloader)):\n",
    "            batch = {k: v.to(gpu) for k, v in batch.items()}\n",
    "            pred = model(batch['img'], batch['cat_features'], batch['num_features'])\n",
    "            \n",
    "            valid_preds += [pred.clone().detach().cpu()]\n",
    "        \n",
    "        for idx, batch in tqdm(enumerate(test_dataloader)):\n",
    "            batch = {k: v.to(gpu) for k, v in batch.items()}\n",
    "            pred = model(batch['img'], batch['cat_features'], batch['num_features'])\n",
    "            \n",
    "            test_preds += [pred.clone().detach().cpu()]\n",
    "            \n",
    "    valid_preds_fold[fold] = valid_preds\n",
    "    test_preds_fold[fold] = test_preds\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fold : 0\n",
      "start fold : 1\n",
      "start fold : 2\n",
      "start fold : 3\n",
      "start fold : 4\n"
     ]
    }
   ],
   "source": [
    "b0_oof = np.zeros(len(df_train))\n",
    "\n",
    "for fold in config.train_params.selected_folds:\n",
    "    print('start fold :', fold)\n",
    "\n",
    "    b0_oof[splits[fold][1]] = torch.sigmoid(torch.tensor(np.concatenate(valid_preds_fold[fold]))).numpy()[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7979927277381985"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(\n",
    "        df_train['N_category'],\n",
    "        (b0_oof).round(),\n",
    "        average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8349800325839427"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(\n",
    "        df_train['N_category'],\n",
    "        (b0_oof*.2 + cat_oof*.8).round(),\n",
    "        average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 +np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['N_category'] = np.mean([sigmoid(np.concatenate(p)[:, 1]) for p in test_preds_fold.values()], 0).round()\n",
    "\n",
    "# nn_we = 0.2\n",
    "# cat_we = 0.8\n",
    "# sub['N_category'] = (np.mean([sigmoid(np.concatenate(p)[:, 1]) for p in test_preds_fold.values()], 0)*nn_we + cat_pred*cat_we).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../submit/tf_efficientnetv2_l_in21k_aug9.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_oof = np.load('../submit/cat_oof.npy')\n",
    "cat_pred = np.load('../submit/cat_preds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('../submit/best.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ray_pl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65e69a851bb5a179bf7435f637a9dd3f2d67ff7b0f73f2181633d998f9e977c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
